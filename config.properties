bootstrapServerAddress = 172.18.120.27:9092,172.18.120.28:9092,172.18.120.29:9092,172.18.120.30:9092
ZkAddress = 172.18.120.30
kafkaTopic = test09
#kafka是否开启压缩,默认是none，可选gzip，snappy，lz4，zstd
compressionCodec = snappy

#column数量（包含rowkey）
columnNum = 6

kafkaSecuredMode = 
saslKerberosServiceName = 
principal = 
keytabPath = 

kafkaRefactor = 3
kafkaConsumerTimeOut = 30000
intervalFlushTime = 100

#是否永久执行数据插入
isInsertUnlimited = true

#如果执行有限制的话设置插入的数据量
maxNum = 100000000

#是否开启argo插入数据时的性能计算
startThroughputCalculate = true

#每隔多少条打印一条日志
ThroughputCalculateInterval = 100000

#是否开启限流
isLimiting = false

#限制tps
tps = 1000000

#是否是分区表
isPartition = false

#哪些属性是被分区的（对应属性的index）
partitionIndex = 3

#判断生成的数据是否是kundb数据
isKunDB = false

#gg.handler.tdthandler.SaslKerberosServiceName=kafka
#gg.handler.tdthandler.Principal=admin@SGIDCTDH
#gg.handler.tdthandler.KeytabPath=/root/kafka.keytab
#gg.handler.tdthandler.PrincipalStream=admin@SGIDCTDH
#gg.handler.tdthandler.KeytabPathStream=/etc/slipstream2/conf/kafka.keytab

